{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfaa0bc4",
   "metadata": {},
   "source": [
    "# Simple Q-Learning Example with PyTorch\n",
    "This notebook introduces the fundamental concepts of Q-learning, a foundational reinforcement learning algorithm. It demonstrates a minimal Q-learning agent using PyTorch in a simple gridworld environment. The example is suitable for beginners and includes detailed explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3236cbbc",
   "metadata": {},
   "source": [
    "## What is Q-Learning?\n",
    "Q-learning is a value-based reinforcement learning algorithm. It learns the optimal action-value function (Q-function) for each state-action pair, allowing the agent to select actions that maximize expected cumulative reward. Unlike the bandit problem, Q-learning is designed for environments with states and transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34414ef4",
   "metadata": {},
   "source": [
    "## Minimal Q-Learning Example: Gridworld\n",
    "We use a simple 1D gridworld with 5 states. The agent starts at the leftmost state and aims to reach the rightmost state (goal). At each step, the agent can move left or right. The episode ends when the agent reaches the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0685f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Define the environment: 1D gridworld with 5 states (0 to 4)\n",
    "class GridWorld:\n",
    "    def __init__(self, n_states=5):\n",
    "        self.n_states = n_states\n",
    "        self.start_state = 0\n",
    "        self.goal_state = n_states - 1\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "    def step(self, action):\n",
    "        # action: 0=left, 1=right\n",
    "        if action == 0:\n",
    "            next_state = max(0, self.state - 1)\n",
    "        else:\n",
    "            next_state = min(self.n_states - 1, self.state + 1)\n",
    "        reward = 1 if next_state == self.goal_state else 0\n",
    "        done = next_state == self.goal_state\n",
    "        self.state = next_state\n",
    "        return next_state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab5c11e",
   "metadata": {},
   "source": [
    "## Q-Learning Agent\n",
    "The agent maintains a Q-table (state-action values) and updates it using the Q-learning update rule:\n",
    "\n",
    "Q(s, a) ← Q(s, a) + α [r + γ * max Q(s', a') - Q(s, a)]\n",
    "\n",
    "where:\n",
    "- s: current state\n",
    "- a: action taken\n",
    "- r: reward received\n",
    "- s': next state\n",
    "- α: learning rate\n",
    "- γ: discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd7dd11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, n_states, n_actions, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "        self.q_table = torch.zeros(n_states, n_actions)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "    def select_action(self, state):\n",
    "        # Epsilon-greedy policy\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            return torch.argmax(self.q_table[state]).item()\n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        # Q-learning update rule\n",
    "        max_q_next = 0 if done else torch.max(self.q_table[next_state]).item()\n",
    "        td_target = reward + self.gamma * max_q_next\n",
    "        td_error = td_target - self.q_table[state, action]\n",
    "        self.q_table[state, action] += self.alpha * td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafabab0",
   "metadata": {},
   "source": [
    "## Training Loop (Forward Path)\n",
    "The agent interacts with the environment for several episodes, updating its Q-table after each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6468bb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      " tensor([[0.5985, 0.7290],\n",
      "        [0.6080, 0.8100],\n",
      "        [0.6479, 0.9000],\n",
      "        [0.6607, 1.0000],\n",
      "        [0.0000, 0.0000]])\n",
      "Average steps to goal: 4.35\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld(n_states=5)\n",
    "agent = QLearningAgent(n_states=5, n_actions=2, alpha=0.1, gamma=0.9, epsilon=0.2)\n",
    "episodes = 200\n",
    "steps_per_episode = []\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        step_count += 1\n",
    "    steps_per_episode.append(step_count)\n",
    "\n",
    "print(\"Learned Q-table:\\n\", agent.q_table)\n",
    "print(\"Average steps to goal:\", np.mean(steps_per_episode[-20:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e0497",
   "metadata": {},
   "source": [
    "## Summary\n",
    "This notebook introduced the basics of Q-learning and demonstrated a simple Q-learning agent solving a gridworld problem. For more advanced RL, consider exploring deep Q-networks (DQN), policy gradients, or actor-critic methods."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
