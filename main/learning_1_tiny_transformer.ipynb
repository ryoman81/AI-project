{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e01a7eb6",
   "metadata": {},
   "source": [
    "# Tiny Transformer\n",
    "This is a self-driven study note of building a tiny transformer that predicts next word given the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea67e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4254c3",
   "metadata": {},
   "source": [
    "## Part 1: Build a transformer model\n",
    "### Step 1: Token Embedding + Sinusoidal Positional Encoding\n",
    "- We map each word (token index) to a dense vector using an nn.Embedding.\n",
    "- We add positional encoding to give the model information about word order.\n",
    "- Each position `pos` in the sequence gets a deterministic embedding using sine and cosine functions of different frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cc20623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_seq_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Create a matrix of shape (max_seq_len, embed_dim)\n",
    "        pe = torch.zeros(max_seq_len, embed_dim)\n",
    "        # Position indices: (max_seq_len, 1)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "\n",
    "        # Compute the denominator term for each dimension (even indices only)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "\n",
    "        # Apply sine to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # Add a batch dimension: (1, max_seq_len, embed_dim)\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as a buffer to exclude from model parameters (not trainable)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        seq_len = x.size(1)\n",
    "        # Add the positional encoding (broadcasted over batch)\n",
    "        return x + self.pe[:, :seq_len, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ff3c0c",
   "metadata": {},
   "source": [
    "#### A common layer of token embedding + psitional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "703444e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbeddingWithPE(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len):\n",
    "        super().__init__()\n",
    "        # Token embedding layer (maps word indices to dense vectors)\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Sinusoidal positional encoding (not learned)\n",
    "        self.pos_encoder = SinusoidalPositionalEncoding(embed_dim, max_seq_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len)\n",
    "        token_embeddings = self.token_embed(x)  # -> (batch_size, seq_len, embed_dim)\n",
    "        return self.pos_encoder(token_embeddings)  # Add positional info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf7c9d",
   "metadata": {},
   "source": [
    "### Step 2: Multi-Head Self-Attention layer\n",
    "Multi-Head Self-Attention (MHSA) allows the model to focus on different parts of the sequence in parallel using multiple attention \"heads\". For each head, we compute attention using separate linear projections of the input:\n",
    "  - Queries (Q), Keys (K), and Values (V) are all derived from the input `x`.\n",
    "  - Attention(Q, K, V) = `softmax(QKᵀ / sqrt(d_k)) * V`\n",
    "    - Q, K, V are matrices of shape (batch_size, num_heads, seq_len, head_dim)\n",
    "    - d_k = head_dim, which is typically embed_dim / num_heads\n",
    "\n",
    "After computing attention output for all heads, we:\n",
    "  1. Concatenate the outputs from all heads.\n",
    "  2. Apply a final linear projection to get back to the original embedding dimension.\n",
    "\n",
    "Input/Output shape:\n",
    "  - Input: (batch_size, seq_len, embed_dim)\n",
    "  - Output: (batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dab5a534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Linear layers for Q, K, V projections (all heads at once)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        # Output projection\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_len, embed_dim)\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project to Q, K, V\n",
    "        q = self.q_proj(x)  # (B, T, C)\n",
    "        k = self.k_proj(x)\n",
    "        v = self.v_proj(x)\n",
    "\n",
    "        # Split heads: (B, T, num_heads, head_dim) → (B, num_heads, T, head_dim)\n",
    "        q = q.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, T, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        attn_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, num_heads, T, T)\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)  # (B, num_heads, T, T)\n",
    "        attn_output = torch.matmul(attn_weights, v)  # (B, num_heads, T, head_dim)\n",
    "\n",
    "        # Concatenate heads: (B, num_heads, T, head_dim) → (B, T, C)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(B, T, C)\n",
    "\n",
    "        # Final linear projection\n",
    "        return self.out_proj(attn_output)  # (B, T, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a84746",
   "metadata": {},
   "source": [
    "### Step 3: Transformer block (LayerNorm + Residual + FeedForward)\n",
    "A Transformer block contains two main sub-layers:\n",
    "  1. **Multi-Head Self-Attention (MHSA)** with residual connection and LayerNorm.\n",
    "  2. **FeedForward Network (FFN)** with residual connection and LayerNorm.\n",
    "\n",
    "**LayerNorm** helps stabilize training and is applied **before** each sub-layer (\"Pre-LN\" style).\n",
    "\n",
    "**Residual connections** (x + sublayer(x)) allow gradient flow and faster training.\n",
    "\n",
    "**FeedForward Network** is applied position-wise and consists of:\n",
    "  - Linear -> ReLU -> Linear\n",
    "\n",
    "Shapes:\n",
    "  - Input/Output: (batch_size, seq_len, embed_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb590a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),  # GELU also commonly used\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ff = FeedForward(embed_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pre-norm and residual connection for MHSA\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        # Pre-norm and residual connection for FFN\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397bdc97",
   "metadata": {},
   "source": [
    "### Step 4: Full Transformer Model\n",
    "This is a **decoder-only Transformer** for autoregressive next-word prediction. It consists of:\n",
    "  1. **Embedding layer**: token embeddings + positional encoding\n",
    "  2. **Stacked Transformer blocks**\n",
    "  3. **Final linear layer**: maps hidden states to vocabulary logits\n",
    "\n",
    "Shape:\n",
    "- Input: token indices (batch_size, seq_len)  \n",
    "- Output: logits (batch_size, seq_len, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ab564d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, max_seq_len, num_heads, ff_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Token embedding + sinusoidal positional encoding\n",
    "        self.embed = TokenEmbeddingWithPE(vocab_size, embed_dim, max_seq_len)\n",
    "\n",
    "        # Stack of Transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final layer normalization before output\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Output projection to vocab size for predicting next word\n",
    "        self.head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len) - input token indices\n",
    "        Returns: (batch_size, seq_len, vocab_size) - output logits for each position\n",
    "        \"\"\"\n",
    "        # Embed tokens + add sinusoidal positional encoding\n",
    "        x = self.embed(x)  # shape: (B, T, embed_dim)\n",
    "        # Apply each Transformer block sequentially\n",
    "        for block in self.blocks:\n",
    "            x = block(x)  # shape remains (B, T, embed_dim)\n",
    "        # Normalize final output\n",
    "        x = self.norm(x)  # shape: (B, T, embed_dim)\n",
    "\n",
    "        # Project to logits over vocabulary\n",
    "        logits = self.head(x)  # shape: (B, T, vocab_size)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec7772",
   "metadata": {},
   "source": [
    "## Part 2: Train model\n",
    "### Step 1: Toy Corpus and Tokenization\n",
    "We use a toy dataset with a minimal vocabulary for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1d4be6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"\n",
    "Once upon a time there was a small cat who loved to chase mice and sleep in the sun.\n",
    "The cat lived in a cozy house with a kind old woman who gave it food and warm milk.\n",
    "Sometimes the cat would jump onto the windowsill and watch the birds fly by.\n",
    "At night, the cat curled up on a soft pillow and dreamed of magical forests.\n",
    "One day, the cat met a clever fox who told stories about the world beyond the forest.\n",
    "They became friends and went on small adventures together under the moonlight.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8c52f3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_toy_corpus_data():\n",
    "    # Split into words and build vocab\n",
    "    words = corpus.split()\n",
    "    vocab = sorted(set(words))\n",
    "\n",
    "    # Mapping from word ↔ index\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "    itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "    # Encode full corpus as list of token indices\n",
    "    data = [stoi[w] for w in words]\n",
    "\n",
    "    return data, vocab, stoi, itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e40e850",
   "metadata": {},
   "source": [
    "This is an alternative dataset, tiny shakespeare, which is 1MB size and suitable for small size decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f832f70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_tiny_shakespeare_data(level=\"char\"):\n",
    "    # Load the tiny Shakespeare dataset\n",
    "    with open('data_tiny_shakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    if level == \"word\":\n",
    "        tokens = text.split()\n",
    "    elif level == \"char\":\n",
    "        tokens = list(text)\n",
    "    else:\n",
    "        raise ValueError(\"level must be 'word' or 'char'\")\n",
    "\n",
    "    # Create vocab\n",
    "    vocab = sorted(set(tokens))\n",
    "    stoi = {w: i for i, w in enumerate(vocab)}\n",
    "    itos = {i: w for w, i in stoi.items()}\n",
    "\n",
    "    # Convert words to token indices\n",
    "    data = [stoi[w] for w in tokens]\n",
    "    \n",
    "    return data, vocab, stoi, itos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e562d21b",
   "metadata": {},
   "source": [
    "#### Batch generation function and define the sequence length\n",
    "We’ll use a fixed context window (e.g., seq_len = 4) and generate training samples as:\n",
    "- Input: first n tokens\n",
    "- Target: next n tokens (shifted by 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6af75274",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, batch_size, seq_len, device='cpu'):\n",
    "    \"\"\"\n",
    "    Randomly sample batches of (input, target) pairs from the corpus.\n",
    "\n",
    "    Returns:\n",
    "        x: (batch_size, seq_len) - input token indices\n",
    "        y: (batch_size, seq_len) - next-token targets\n",
    "    \"\"\"\n",
    "    ix = torch.randint(0, len(data) - seq_len - 1, (batch_size,))\n",
    "    x = torch.stack([torch.tensor(data[i:i+seq_len]) for i in ix])\n",
    "    y = torch.stack([torch.tensor(data[i+1:i+1+seq_len]) for i in ix])\n",
    "    return x.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f48f7e5",
   "metadata": {},
   "source": [
    "### Step 2: Training loop\n",
    "The basic training loop that includes\n",
    "1. Define the hyper-parameters\n",
    "2. Define the loss function and optimizer\n",
    "3. Train model in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "52254b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 128\n",
    "ff_dim = 512\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "max_seq_len = 128\n",
    "batch_size = 16\n",
    "num_iters = 10000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7505dfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss: 4.3224\n",
      "Step 100, loss: 2.4791\n",
      "Step 200, loss: 2.2092\n",
      "Step 300, loss: 0.4823\n",
      "Step 400, loss: 0.0959\n",
      "Step 500, loss: 0.0458\n",
      "Step 600, loss: 0.0303\n",
      "Step 700, loss: 0.0291\n",
      "Step 800, loss: 0.0225\n",
      "Step 900, loss: 0.0200\n",
      "Step 1000, loss: 0.0206\n",
      "Step 1100, loss: 0.0166\n",
      "Step 1200, loss: 0.0173\n",
      "Step 1300, loss: 0.0173\n",
      "Step 1400, loss: 0.0224\n",
      "Step 1500, loss: 0.0159\n",
      "Step 1600, loss: 0.0229\n",
      "Step 1700, loss: 0.0211\n",
      "Step 1800, loss: 0.0142\n",
      "Step 1900, loss: 0.0207\n",
      "Step 2000, loss: 0.0159\n",
      "Step 2100, loss: 0.0163\n",
      "Step 2200, loss: 0.0243\n",
      "Step 2300, loss: 0.0135\n",
      "Step 2400, loss: 0.0172\n",
      "Step 2500, loss: 0.0174\n",
      "Step 2600, loss: 0.0236\n",
      "Step 2700, loss: 0.0271\n",
      "Step 2800, loss: 0.0256\n",
      "Step 2900, loss: 0.0203\n",
      "Step 3000, loss: 0.0183\n",
      "Step 3100, loss: 0.0205\n",
      "Step 3200, loss: 0.0152\n",
      "Step 3300, loss: 0.0209\n",
      "Step 3400, loss: 0.0197\n",
      "Step 3500, loss: 0.0230\n",
      "Step 3600, loss: 0.0194\n",
      "Step 3700, loss: 0.0219\n",
      "Step 3800, loss: 0.0159\n",
      "Step 3900, loss: 0.0242\n",
      "Step 4000, loss: 0.0168\n",
      "Step 4100, loss: 0.0250\n",
      "Step 4200, loss: 0.0234\n",
      "Step 4300, loss: 0.0172\n",
      "Step 4400, loss: 0.0220\n",
      "Step 4500, loss: 0.0215\n",
      "Step 4600, loss: 0.0180\n",
      "Step 4700, loss: 0.0173\n",
      "Step 4800, loss: 0.0155\n",
      "Step 4900, loss: 0.0202\n",
      "Step 5000, loss: 0.0149\n",
      "Step 5100, loss: 0.0182\n",
      "Step 5200, loss: 0.0251\n",
      "Step 5300, loss: 0.0170\n",
      "Step 5400, loss: 0.0177\n",
      "Step 5500, loss: 0.0177\n",
      "Step 5600, loss: 0.0173\n",
      "Step 5700, loss: 0.0124\n",
      "Step 5800, loss: 0.0217\n",
      "Step 5900, loss: 0.0166\n",
      "Step 6000, loss: 0.0142\n",
      "Step 6100, loss: 0.0222\n",
      "Step 6200, loss: 0.0182\n",
      "Step 6300, loss: 0.0176\n",
      "Step 6400, loss: 0.0230\n",
      "Step 6500, loss: 0.0169\n",
      "Step 6600, loss: 0.0251\n",
      "Step 6700, loss: 0.0188\n",
      "Step 6800, loss: 0.0193\n",
      "Step 6900, loss: 0.0177\n",
      "Step 7000, loss: 0.0212\n",
      "Step 7100, loss: 0.0126\n",
      "Step 7200, loss: 0.0186\n",
      "Step 7300, loss: 0.0208\n",
      "Step 7400, loss: 0.0171\n",
      "Step 7500, loss: 0.0142\n",
      "Step 7600, loss: 0.0151\n",
      "Step 7700, loss: 0.0195\n",
      "Step 7800, loss: 0.0123\n",
      "Step 7900, loss: 0.0219\n",
      "Step 8000, loss: 0.0138\n",
      "Step 8100, loss: 0.0196\n",
      "Step 8200, loss: 0.0212\n",
      "Step 8300, loss: 0.0177\n",
      "Step 8400, loss: 0.0181\n",
      "Step 8500, loss: 0.0183\n",
      "Step 8600, loss: 0.0225\n",
      "Step 8700, loss: 0.0146\n",
      "Step 8800, loss: 0.0193\n",
      "Step 8900, loss: 0.0114\n",
      "Step 9000, loss: 0.0221\n",
      "Step 9100, loss: 0.0167\n",
      "Step 9200, loss: 0.0132\n",
      "Step 9300, loss: 0.0152\n",
      "Step 9400, loss: 0.0158\n",
      "Step 9500, loss: 0.0189\n",
      "Step 9600, loss: 0.0176\n",
      "Step 9700, loss: 0.0136\n",
      "Step 9800, loss: 0.0196\n",
      "Step 9900, loss: 0.0126\n"
     ]
    }
   ],
   "source": [
    "# Add MPS device support for MacOS M-series GPUs\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Use data\n",
    "# data, vocab, stoi, itos = use_toy_corpus_data()\n",
    "data, vocab, stoi, itos = use_tiny_shakespeare_data()\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Model\n",
    "model = TransformerLanguageModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embed_dim=embed_dim,\n",
    "    max_seq_len=max_seq_len,\n",
    "    num_heads=num_heads,\n",
    "    ff_dim=ff_dim,\n",
    "    num_layers=num_layers\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for step in range(num_iters):\n",
    "    model.train()\n",
    "\n",
    "    x, y = get_batch(data, batch_size, max_seq_len, device=device)\n",
    "    logits = model(x)  # (B, T, vocab_size)\n",
    "\n",
    "    # Reshape for loss: flatten batch and sequence dimensions\n",
    "    loss = loss_fn(logits.view(-1, vocab_size), y.view(-1))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if step % eval_interval == 0:\n",
    "        print(f\"Step {step}, loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8e6990",
   "metadata": {},
   "source": [
    "### Step 3: generating text\n",
    "TO BE COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16f06449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, prompt, max_new_tokens, stoi, itos, prediction_type=\"char\",\n",
    "                  method=\"greedy\", temperature=1.2, top_k=40):\n",
    "    \"\"\"\n",
    "    Modular text generation function supporting greedy, temperature, and top-k sampling.\n",
    "\n",
    "    Args:\n",
    "        model: Trained language model.\n",
    "        prompt: String, starting text to generate from.\n",
    "        max_new_tokens: Number of tokens to generate.\n",
    "        stoi: Dict[str, int], word to index mapping.\n",
    "        itos: Dict[int, str], index to word mapping.\n",
    "        method: \"greedy\", \"temperature\", or \"top-k\"\n",
    "        temperature: Sampling temperature for 'temperature' and 'top-k' methods.\n",
    "        top_k: Integer, how many top tokens to keep for 'top-k' sampling.\n",
    "\n",
    "    Returns:\n",
    "        List of words (including start prompt + generated words)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    words = prompt.split()\n",
    "    if prediction_type == \"char\":\n",
    "        start_tokens = [stoi.get(ch, stoi.get(\"<unk>\", 0)) for ch in prompt]\n",
    "    else:\n",
    "        start_tokens = [stoi.get(w, stoi.get(\"<unk>\", 0)) for w in words]\n",
    "    tokens = start_tokens[:]\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        x = torch.tensor([tokens[-max_seq_len:]], dtype=torch.long).to(device)\n",
    "\n",
    "        logits = model(x)  # (1, seq_len, vocab_size)\n",
    "        logits_last = logits[0, -1]  # get logits for last token\n",
    "\n",
    "        if method == \"greedy\":\n",
    "            next_token = torch.argmax(logits_last, dim=-1).item()\n",
    "\n",
    "        elif method == \"temperature\":\n",
    "            scaled_logits = logits_last / temperature\n",
    "            probs = F.softmax(scaled_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        elif method == \"top-k\":\n",
    "            # Keep only top_k logits\n",
    "            scaled_logits = logits_last / temperature\n",
    "            top_k_logits, top_k_indices = torch.topk(scaled_logits, k=top_k)\n",
    "\n",
    "            # Sample from top-k softmax\n",
    "            top_k_probs = F.softmax(top_k_logits, dim=-1)\n",
    "            sample_idx = torch.multinomial(top_k_probs, num_samples=1).item()\n",
    "            next_token = top_k_indices[sample_idx].item()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown sampling method: {method}\")\n",
    "\n",
    "        tokens.append(next_token)\n",
    "\n",
    "    generated = [itos[t] for t in tokens[len(start_tokens):]]\n",
    "    return ' '.join(generated) if prediction_type == \"word\" else ''.join(generated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "80b2192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \n",
      "ROMEO: \n",
      "\n",
      "Generated: \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "FFFFFF\n",
      "FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFEEEEDDYJIIOOOBEEEOOxEEEEEOxEEEEEEEEEEEEEEEEEEEEEEEMEDENES:EEEEEETEO:\n",
      "My hingrild lo owredeupespedred cind.\n",
      "\n",
      "GORWAVEce.\n",
      "Yomane: hoit\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "ROMEO: \n",
    "\"\"\"\n",
    "method = \"top-k\"  # Change to \"greedy\" or \"temperature\" for different methods\n",
    "generated_words = generate_text(model, prompt, max_new_tokens=200, stoi=stoi, itos=itos, method=method, temperature=1.2, top_k=40)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated:\", generated_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e3ceb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
